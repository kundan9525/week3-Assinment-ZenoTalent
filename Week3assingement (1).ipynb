{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "ips-Gk4PPkE-",
        "outputId": "3b373146-5f80-4bee-df50-62f8eec7868e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Unigrams: [('br', 114890), ('movie', 83523), ('film', 74459), ('one', 51028), ('like', 38992), ('good', 28570), ('even', 24576), ('would', 24024), ('time', 23269), ('really', 22951)]\n",
            "Top 10 Bigrams: [('br br', 14098), ('ever seen', 2528), ('br film', 2481), ('br movie', 2430), ('ive seen', 2203), ('special effects', 2145), ('dont know', 2056), ('itbr br', 2009), ('even though', 1940), ('one best', 1864)]\n",
            "Top 10 Trigram: [('ive ever seen', 984), ('dont waste time', 366), ('worst movie ever', 365), ('one worst movies', 313), ('movie ever seen', 299), ('movie br br', 298), ('br br movie', 297), ('br br film', 297), ('dont get wrong', 269), ('new york city', 252)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6874566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mword_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalatte\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#  Text Preprocessing\n",
        "\n",
        "# Load CSV file\n",
        "data = pd.read_csv('imdb_dataset.csv')\n",
        "data.head()\n",
        "\n",
        "# preprocessing function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "  text = text.lower()  # Lowercase\n",
        "  text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove punctuation and numbers\n",
        "  tokens = word_tokenize(text) # Tokenize\n",
        "  tokens = [word for word in tokens if word not in stop_words]  # Remove stopewords\n",
        "  return tokens\n",
        "\n",
        "# Apply to your review column\n",
        "data['processed_review'] = data['review'].apply(preprocess_text)\n",
        "data.head()\n",
        "\n",
        "# Generate n-grams\n",
        "\n",
        "# Helper function for n-grams\n",
        "def generate_ngrams(tokens, n):\n",
        "  return zip(*[tokens[i:] for i in range(n)])\n",
        "\n",
        "# Count top N n-grams\n",
        "def get_top_ngrams(corpus, ngram=1, top=10):\n",
        "  all_ngrams = []\n",
        "  for tokens in corpus:\n",
        "    ngrams = generate_ngrams(tokens, ngram)\n",
        "    all_ngrams += [' '.join(ngram) for ngram in ngrams]\n",
        "  return Counter(all_ngrams).most_common(top)\n",
        "\n",
        "# Get top 10 unigrams,bigrams,trigrams\n",
        "top_unigrams = get_top_ngrams(data['processed_review'], ngram=1)\n",
        "top_bigrams = get_top_ngrams(data['processed_review'], ngram=2)\n",
        "top_trigrams = get_top_ngrams(data['processed_review'], ngram=3)\n",
        "\n",
        "print(\"Top 10 Unigrams:\", top_unigrams)\n",
        "print(\"Top 10 Bigrams:\", top_bigrams)\n",
        "print(\"Top 10 Trigram:\", top_trigrams)\n",
        "\n",
        "# Word Frequency Visualization\n",
        "\n",
        "all_words = [word for tokens in data['processed_review'] for word in tokens]\n",
        "word_freq = Counter(all_words)\n",
        "most_common_words = word_freq.most_common(20)\n",
        "\n",
        "# Plot\n",
        "words, freqs = zip(*word_freq)\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=list(words), y=list(freqs), palatte='viridis')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Words')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Bag-of-words vectorization\n",
        "\n",
        "data['processed_review_str'] = data['processed_review'].apply(lambda x: ' '.join(x))\n",
        "#Creat bag of words\n",
        "vectorizer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(data['processed_review_str'])\n",
        "\n",
        "print(\"Shape of Sparse Matrix:\", x.shape)\n",
        "print(\"Sample feature Name:\", vectorizer.get_feature_names_out()[:10])\n"
      ]
    }
  ]
}